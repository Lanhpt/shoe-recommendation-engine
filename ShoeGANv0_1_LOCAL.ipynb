{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_nHsuaPAXsZ"
   },
   "source": [
    "# TFGAN Tutorial\n",
    "\n",
    "## Authors: Joel Shor, joel-shor\n",
    "\n",
    "### More complex examples, see [`tensorflow/models/gan`](https://github.com/tensorflow/models/tree/master/research/gan)\n",
    "\n",
    "\n",
    "This notebook will walk you through the basics of using [TFGAN](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan) to define, train, and evaluate Generative Adversarial Networks. We describe the library's core features as well as some extra features. This colab assumes a familiarity with TensorFlow's Python API. For more on TensorFlow, please see [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n",
    "\n",
    "Please note that running on GPU will significantly speed up the training steps, but is not required.\n",
    "\n",
    "Last update: 2018-02-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvnnRxw4AXsb"
   },
   "source": [
    "## Table of Contents\n",
    "<a href='#installation_and_setup'>Installation and Setup</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#download_data'>Download Data</a><br>\n",
    "<a href='#unconditional_example'>Unconditional GAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#unconditional_train'>Train and evaluation</a><br>\n",
    "<a href='#ganestimator_example'>GANEstimator example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_train'>Train</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#ganestimator_eval'>Eval</a><br>\n",
    "<a href='#conditional_example'>Conditional GAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#conditional_train'>Train and evaluation</a><br>\n",
    "<a href='#infogan_example'>InfoGAN example</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_input'>Input pipeline</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_model'>Model</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_loss'>Loss</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#infogan_train'>Train and evaluation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNx4HAKyAXsc"
   },
   "source": [
    "<a id='installation_and_setup'></a>\n",
    "## Installation and setup\n",
    "\n",
    "To make sure that your version of TensorFlow has TFGAN included, run\n",
    "\n",
    "```\n",
    "python -c \"import tensorflow.contrib.gan as tfgan\"\n",
    "```\n",
    "\n",
    "You also need to install the TFGAN models library.\n",
    "\n",
    "To check that these two steps work, execute the [`Imports`](#imports) cell. If it complains about unknown modules, restart the notebook after moving to the TFGAN models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eX0pb-goAXsd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
      "  from ._conv import register_converters as _register_converters\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tensorflow.contrib.gan as tfgan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8c9LjIbfAXsy"
   },
   "outputs": [],
   "source": [
    "# Make TFGAN models and TF-Slim models discoverable.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "#make sure we're in models/research/gan\n",
    "print(os.getcwd())\n",
    "\n",
    "os.chdir('../tf_experiment/models/research/gan')\n",
    "\n",
    "# This is needed since the notebook is stored in the `tensorflow/models/gan` folder.\n",
    "sys.path.append('..')\n",
    "sys.path.append(os.path.join('..', 'slim'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQ0Qw1mPAXs7"
   },
   "source": [
    "<a id='imports'></a>\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUfmCS3vaxLn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "MV-KH-2bAXs7",
    "outputId": "0103588f-2a75-4898-ff76-82df25142177"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/griggles/Documents/FLATIRON/FINAL PROJECT/Fashion/shoes\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'slim.datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1c351b4cb273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# TFGAN MNIST examples from `tensorflow/models`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FLATIRON/FINAL PROJECT/Fashion/tf_experiment/models/research/gan/mnist/data_provider.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_factory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mslim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'slim.datasets'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Main TFGAN library.\n",
    "tfgan = tf.contrib.gan\n",
    "\n",
    "# TFGAN MNIST examples from `tensorflow/models`.\n",
    "from mnist import data_provider\n",
    "from mnist import util\n",
    "\n",
    "# TF-Slim data provider.\n",
    "\n",
    "####Â need to figure out how to give shoes to this thing here vvvvvvv\n",
    "from datasets import download_and_convert_mnist\n",
    "\n",
    "# Shortcuts for later.\n",
    "queues = tf.contrib.slim.queues\n",
    "layers = tf.contrib.layers\n",
    "ds = tf.contrib.distributions\n",
    "framework = tf.contrib.framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8Jtw5yfAXtP"
   },
   "source": [
    "### Common functions\n",
    "\n",
    "These functions are used by many examples, so we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zf3OybWTAXtR"
   },
   "outputs": [],
   "source": [
    "leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "  \n",
    "\n",
    "def visualize_training_generator(train_step_num, start_time, data_np):\n",
    "    \"\"\"Visualize generator outputs during training.\n",
    "    \n",
    "    Args:\n",
    "        train_step_num: The training step number. A python integer.\n",
    "        start_time: Time when training started. The output of `time.time()`. A\n",
    "            python float.\n",
    "        data: Data to plot. A numpy array, most likely from an evaluated TensorFlow\n",
    "            tensor.\n",
    "    \"\"\"\n",
    "    print('Training step: %i' % train_step_num)\n",
    "    time_since_start = (time.time() - start_time) / 60.0\n",
    "    print('Time since start: %f m' % time_since_start)\n",
    "    print('Steps per min: %f' % (train_step_num / time_since_start))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(data_np), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_digits(tensor_to_visualize):\n",
    "    \"\"\"Visualize an image once. Used to visualize generator before training.\n",
    "    \n",
    "    Args:\n",
    "        tensor_to_visualize: An image tensor to visualize. A python Tensor.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with queues.QueueRunners(sess):\n",
    "            images_np = sess.run(tensor_to_visualize)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.squeeze(images_np), cmap='gray')\n",
    "\n",
    "def evaluate_tfgan_loss(gan_loss, name=None):\n",
    "    \"\"\"Evaluate GAN losses. Used to check that the graph is correct.\n",
    "    \n",
    "    Args:\n",
    "        gan_loss: A GANLoss tuple.\n",
    "        name: Optional. If present, append to debug output.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with queues.QueueRunners(sess):\n",
    "            gen_loss_np = sess.run(gan_loss.generator_loss)\n",
    "            dis_loss_np = sess.run(gan_loss.discriminator_loss)\n",
    "    if name:\n",
    "        print('%s generator loss: %f' % (name, gen_loss_np))\n",
    "        print('%s discriminator loss: %f'% (name, dis_loss_np))\n",
    "    else:\n",
    "        print('Generator loss: %f' % gen_loss_np)\n",
    "        print('Discriminator loss: %f'% dis_loss_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1Rq9nKIAXtU"
   },
   "source": [
    "<a id='download_data'></a>\n",
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DjexOJbAXtW"
   },
   "outputs": [],
   "source": [
    "MNIST_DATA_DIR = 'tmp/mnist'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not tf.gfile.Exists(MNIST_DATA_DIR):\n",
    "    tf.gfile.MakeDirs(MNIST_DATA_DIR)\n",
    "\n",
    "    \n",
    "    ####PUT SHOES IN HERE vvv\n",
    "    \n",
    "    #this is downloading mnist data and converting to a .tfrecord...\n",
    "    \n",
    "    #we just converted to a tf record already... so we could just upload that tf \n",
    "    #record... (and put it in the samep place as MNIST_DATA_DIR and then not CALL\n",
    "    #DOWNLOAD AND CONVERT... THEN THE DATA SHOULD BE POINTING AT OUR .tfrecord :)\n",
    "\n",
    "\n",
    "download_and_convert_mnist.run(MNIST_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2rmFQ5kAXtb"
   },
   "source": [
    "<a id='unconditional_example'></a>\n",
    "# Unconditional GAN Example\n",
    "\n",
    "With unconditional GANs, we want a generator network to produce realistic-looking digits. During training, the generator tries to produce realistic-enough digits to 'fool' a discriminator network, while the discriminator tries to distinguish real digits from generated ones. See the paper ['NIPS 2016 Tutorial: Generative Adversarial Networks'](https://arxiv.org/pdf/1701.00160.pdf) by Goodfellow or ['Generative Adversarial Networks'](https://arxiv.org/abs/1406.2661) by Goodfellow et al. for more details.\n",
    "\n",
    "The steps to using TFGAN to set up an unconditional GAN, in the simplest case, are as follows:\n",
    "\n",
    "1. **Model**: Set up the generator and discriminator graphs with a [`GANModel`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L39) tuple. Use [`tfgan.gan_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L64) or create one manually.\n",
    "1. **Losses**: Set up the generator and discriminator losses with a [`GANLoss`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L115) tuple. Use [`tfgan.gan_loss`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L328) or create one manually.\n",
    "1. **Train ops**: Set up TensorFlow ops that compute the loss, calculate the gradients, and update the weights with a [`GANTrainOps`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L128) tuple. Use [`tfgan.gan_train_ops`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L476) or create one manually.\n",
    "1. **Run alternating train loop**: Run the training Ops. This can be done with [`tfgan.gan_train`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L661), or done manually.\n",
    "\n",
    "Each step can be performed by a TFGAN library call, or can be constructed manually for more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44okyzjZAXtc"
   },
   "source": [
    "<a id='unconditional_input'></a>\n",
    "## Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "yT2GiAgKAXtd",
    "outputId": "501ec145-a2ef-4569-d9e7-896b18af602c"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our input pipeline. Pin it to the CPU so that the GPU can be reserved\n",
    "# for forward and backwards propogation.\n",
    "batch_size = 32\n",
    "with tf.device('/cpu:0'):\n",
    "    real_images, _, _ = data_provider.provide_data(\n",
    "        'train', batch_size, MNIST_DATA_DIR)\n",
    "    \n",
    "    \n",
    "    #just fuckin figure out what this is doing ^ and fix it\n",
    "    \n",
    "    print(real_images)\n",
    "\n",
    "# Sanity check that we're getting images.\n",
    "check_real_digits = tfgan.eval.image_reshaper(\n",
    "    real_images[:20,...], num_cols=10)\n",
    "visualize_digits(check_real_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9inbCtSAXt-"
   },
   "source": [
    "<a id='unconditional_model'></a>\n",
    "## Model\n",
    "\n",
    "Set up a [GANModel tuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L39), which defines everything we need to perform GAN training. You can create the tuple with the library functions, or you can manually create one.\n",
    "\n",
    "Define the GANModel tuple using the TFGAN library function.\n",
    "For the simplest case, we need the following:\n",
    "\n",
    "1. A generator function that takes input noise and outputs generated MNIST digits\n",
    "\n",
    "1. A discriminator function that takes images and outputs a probability of  being real or fake\n",
    "\n",
    "1. Real images\n",
    "\n",
    "1. A noise vector to pass to the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyRzwOnHAXuA"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTrJq_nqAXuB"
   },
   "outputs": [],
   "source": [
    "def generator_fn(noise, weight_decay=2.5e-5, is_training=True):\n",
    "    \"\"\"Simple generator to produce MNIST images.\n",
    "    \n",
    "    Args:\n",
    "        noise: A single Tensor representing noise.\n",
    "        weight_decay: The value of the l2 weight decay.\n",
    "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
    "            norm uses the exponential moving average collected from population \n",
    "            statistics.\n",
    "    \n",
    "    Returns:\n",
    "        A generated image in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    with framework.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay)),\\\n",
    "    framework.arg_scope([layers.batch_norm], is_training=is_training,\n",
    "                        zero_debias_moving_mean=True):\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 256)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 256])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        # Make sure that generator output is in the same range as `inputs`\n",
    "        # ie [-1, 1].\n",
    "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-ZQELRAAXul"
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OP2an0miAXum"
   },
   "outputs": [],
   "source": [
    "def discriminator_fn(img, unused_conditioning, weight_decay=2.5e-5,\n",
    "                     is_training=True):\n",
    "    \"\"\"Discriminator network on MNIST digits.\n",
    "    \n",
    "    Args:\n",
    "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
    "        unused_conditioning: The TFGAN API can help with conditional GANs, which\n",
    "            would require extra `condition` information to both the generator and the\n",
    "            discriminator. Since this example is not conditional, we do not use this\n",
    "            argument.\n",
    "        weight_decay: The L2 weight decay.\n",
    "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
    "            norm uses the exponential moving average collected from population \n",
    "            statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Logits for the probability that the image is real.\n",
    "    \"\"\"\n",
    "    with framework.arg_scope(\n",
    "        [layers.conv2d, layers.fully_connected],\n",
    "        activation_fn=leaky_relu, normalizer_fn=None,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        with framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
    "            net = layers.fully_connected(net, 1024, normalizer_fn=layers.batch_norm)\n",
    "        return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHY1rIEVAXuq"
   },
   "source": [
    "### GANModel Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozqdhPImAXur"
   },
   "outputs": [],
   "source": [
    "noise_dims = 64\n",
    "gan_model = tfgan.gan_model(\n",
    "    generator_fn,\n",
    "    discriminator_fn,\n",
    "    real_data=real_images,\n",
    "    generator_inputs=tf.random_normal([batch_size, noise_dims]))\n",
    "\n",
    "# Sanity check that generated images before training are garbage.\n",
    "check_generated_digits = tfgan.eval.image_reshaper(\n",
    "    gan_model.generated_data[:20,...], num_cols=10)\n",
    "visualize_digits(check_generated_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9sD6vrVAXu9"
   },
   "source": [
    "<a id='unconditional_loss'></a>\n",
    "## Losses\n",
    "\n",
    "We next set up the GAN model losses.\n",
    "\n",
    "Loss functions are currently an active area of research. The [losses library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py) provides some well-known or successful loss functions, such as the [original minimax](https://arxiv.org/abs/1406.2661), [Wasserstein](https://arxiv.org/abs/1701.07875) (by Arjovsky et al), and [improved Wasserstein](https://arxiv.org/abs/1704.00028) (by Gulrajani et al) losses. It is easy to add loss functions to the library as they are developed, and you can also pass in a custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hyPSykYFAXu_"
   },
   "outputs": [],
   "source": [
    "# We can use the minimax loss from the original paper.\n",
    "vanilla_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=tfgan.losses.minimax_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss)\n",
    "\n",
    "# We can use the Wasserstein loss (https://arxiv.org/abs/1701.07875) with the \n",
    "# gradient penalty from the improved Wasserstein loss paper \n",
    "# (https://arxiv.org/abs/1704.00028).\n",
    "improved_wgan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    # We make the loss explicit for demonstration, even though the default is \n",
    "    # Wasserstein loss.\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    gradient_penalty_weight=1.0)\n",
    "\n",
    "# We can also define custom losses to use with the rest of the TFGAN framework.\n",
    "def silly_custom_generator_loss(gan_model, add_summaries=False):\n",
    "    return tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "def silly_custom_discriminator_loss(gan_model, add_summaries=False):\n",
    "    return (tf.reduce_mean(gan_model.discriminator_gen_outputs) -\n",
    "            tf.reduce_mean(gan_model.discriminator_real_outputs))\n",
    "custom_gan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=silly_custom_generator_loss,\n",
    "    discriminator_loss_fn=silly_custom_discriminator_loss)\n",
    "\n",
    "# Sanity check that we can evaluate our losses.\n",
    "for gan_loss, name in [(vanilla_gan_loss, 'vanilla loss'), \n",
    "                       (improved_wgan_loss, 'improved wgan loss'), \n",
    "                       (custom_gan_loss, 'custom loss')]:\n",
    "    evaluate_tfgan_loss(gan_loss, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrPxqZYaAXvE"
   },
   "source": [
    "<a id='unconditional_train'></a>\n",
    "## Training and Evaluation\n",
    "\n",
    "### Train Ops\n",
    "In order to train a GAN, we need to train both generator and discriminator networks using some variant of the alternating training paradigm. To do this, we construct a [GANTrainOps tuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/namedtuples.py#L128) either manually or with a library call. We pass it the optimizers that we want to use, as well as any extra arguments that we'd like passed to slim's `create_train_op` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdyJadkWAXvY"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    gan_model,\n",
    "    improved_wgan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhP7B73jAXvg"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "TFGAN provides some standard methods of evaluating generative models. In this example, we use a pre-trained classifier to calculate what is called the 'Inception Score' from [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) (by Salimans et al), which is a combined score of quality and diversity. We also calculate the 'Frechet Inception distance' from [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500) (by Heusel et al), which measures how close the generated image distribution is to the real image distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_EBtUmiAXvy"
   },
   "outputs": [],
   "source": [
    "num_images_to_eval = 500\n",
    "\n",
    "#this is a saved classifier\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './mnist/data/classify_mnist_graph_def.pb'\n",
    "\n",
    "# For variables to load, use the same variable scope as in the train job.\n",
    "with tf.variable_scope('Generator', reuse=True):\n",
    "    eval_images = gan_model.generator_fn(\n",
    "        tf.random_normal([num_images_to_eval, noise_dims]),\n",
    "        is_training=False)\n",
    "\n",
    "# Calculate Inception score.\n",
    "eval_score = util.mnist_score(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "\n",
    "# Calculate Frechet Inception distance.\n",
    "with tf.device('/cpu:0'):\n",
    "    real_images, _, _ = data_provider.provide_data(\n",
    "        'train', num_images_to_eval, MNIST_DATA_DIR)\n",
    "frechet_distance = util.mnist_frechet_distance(\n",
    "    real_images, eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "\n",
    "# Reshape eval images for viewing.\n",
    "generated_data_to_visualize = tfgan.eval.image_reshaper(\n",
    "    eval_images[:20,...], num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KDFjzxVAXv-"
   },
   "outputs": [],
   "source": [
    "#inception score probably just takes a classifier that already works on the right distribution \n",
    "#and sees how it does on this new distribution. Kind of like an analogy between probability\n",
    "#and likelihood. The score represents... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xn3P8E7hAXwO"
   },
   "source": [
    "### Train Steps\n",
    "\n",
    "Now we're ready to train. TFGAN handles the alternating training scheme that arises from the GAN minmax game. It also gives you the option of changing the ratio of discriminator updates to generator updates. Most applications (distributed setting, borg, etc) will use the [`gan_train`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py#L661) function, but we will use a different TFGAN utility and manually run the train ops so we can introspect more.\n",
    "\n",
    "This code block should take about **1 minute** to run on a GPU kernel, and about **8 minutes** on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YvHRl_7AXwP"
   },
   "outputs": [],
   "source": [
    "# We have the option to train the discriminator more than one step for every \n",
    "# step of the generator. In order to do this, we use a `GANTrainSteps` with \n",
    "# desired values. For this example, we use the default 1 generator train step \n",
    "# for every discriminator train step.\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "loss_values, mnist_scores, frechet_distances  = [], [], []\n",
    "\n",
    "with tf.train.SingularMonitoredSession() as sess:\n",
    "    start_time = time.time()\n",
    "    for i in xrange(1601):\n",
    "        cur_loss, _ = train_step_fn(\n",
    "            sess, gan_train_ops, global_step, train_step_kwargs={})\n",
    "        loss_values.append((i, cur_loss))\n",
    "        if i % 200 == 0:\n",
    "            mnist_score, f_distance, digits_np = sess.run(\n",
    "                [eval_score, frechet_distance, generated_data_to_visualize])\n",
    "            mnist_scores.append((i, mnist_score))\n",
    "            frechet_distances.append((i, f_distance))\n",
    "            print('Current loss: %f' % cur_loss)\n",
    "            print('Current MNIST score: %f' % mnist_scores[-1][1])\n",
    "            print('Current Frechet distance: %f' % frechet_distances[-1][1])\n",
    "            visualize_training_generator(i, start_time, digits_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aR1bUa5cAXwr"
   },
   "outputs": [],
   "source": [
    "# Plot the eval metrics over time.\n",
    "plt.title('MNIST Frechet distance per step')\n",
    "plt.plot(*zip(*frechet_distances))\n",
    "plt.figure()\n",
    "plt.title('MNIST Score per step')\n",
    "plt.plot(*zip(*mnist_scores))\n",
    "plt.figure()\n",
    "plt.title('Training loss per step')\n",
    "plt.plot(*zip(*loss_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mnhEXjkD5i8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ShoeGANv0.1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
